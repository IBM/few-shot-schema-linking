import argparse
from typing import List, Dict, Tuple, Optional
from tqdm.auto import tqdm
import random
import json
import numpy as np
import torch

from dec_sl.processing.postprocess import extract_json, flatten_list
from dec_sl.processing.prompting import create_schema_linking_prompt
from dec_sl.utils.file_utils import load_json, write_json
from dec_sl.llm.vllm_base import VLLM
from dec_sl.llm.llm_base import LLM
from dec_sl.llm.llama import Llama
from dec_sl.llm.deepseek import DeepseekCoder
from dec_sl.llm.granite import Granite
from dec_sl.llm.codestral import Codestral
from dec_sl.utils.text_utils import preprocess_evidence, get_demonstration_silimarities
from dec_sl.utils.db_utils import get_matched_content_sequence, get_db_schema_sequence


def combine_json_predictions(json_predictions: List[Dict | None]) -> Dict | None:
    """Combine the json predictions of different sampled output into the final prediction."""
    if not any(json_predictions):
        # If all json predictions are None, return None
        return None

    combined_prediction = {"tables": [], "columns": []}
    for json_prediction in json_predictions:
        if json_prediction is not None:
            # Add current prediction to the unified
            combined_prediction["tables"].extend(json_prediction.get("tables", []))
            combined_prediction["columns"].extend(json_prediction.get("columns", []))

    # Avoid duplicates
    combined_prediction["tables"] = list(set(combined_prediction["tables"]))
    combined_prediction["columns"] = list(set(combined_prediction["columns"]))

    return combined_prediction


def prepare_prompt(
    example: Dict,
    decomposed_question: Optional[str] = None,
) -> Tuple[str, str, str]:
    """Create the LLM prompts for a given example."""
    # Get all necessary data
    if decomposed_question is None:
        nl_question = example["question"]
    else:
        nl_question = decomposed_question

    serialized_schema = get_db_schema_sequence(
        example["schema"], include_fk=False, include_content=False
    )
    evidence = preprocess_evidence(example["evidence"])
    matched_content_sequence = get_matched_content_sequence(example["matched_contents"])

    # Add evidence
    # NOTE: Maybe add this to the prompt?
    if evidence != "":
        nl_question = f"{nl_question} (Notes: {evidence})"

    # Add matched contents
    # NOTE: Maybe add this to the prompt?
    # NOTE: Maybe this could be phrased in a better way?
    if matched_content_sequence != "":
        nl_question = f"{nl_question}\nAlso take into account the following {matched_content_sequence}"

    # Create input prompts
    instruction_prompt, question_prompt = create_schema_linking_prompt(
        serialized_schema, nl_question
    )

    # Load schema and labels
    table_labels = example["table_labels"]
    column_labels = flatten_list(example["column_labels"])
    schema = example["schema"]
    flat_table_names: List[str] = [
        schema_item["table_name"] for schema_item in schema["schema_items"]
    ]
    flat_column_names: List[str] = [
        f"{schema_item['table_name']}.{column_name}"
        for schema_item in schema["schema_items"]
        for column_name in schema_item["column_names"]
    ]

    # Create ground truth for schema linking
    ground_truth = {}
    ground_truth["tables"] = [
        table_name
        for table_name, label in zip(flat_table_names, table_labels)
        if label == 1
    ]
    ground_truth["columns"] = [
        column_name
        for column_name, label in zip(flat_column_names, column_labels)
        if label == 1
    ]

    # Create expected response as it should be generated by the LLM
    expected_response = f"```json\n{json.dumps(ground_truth)}\n```"

    return instruction_prompt, question_prompt, expected_response


def get_prediction(
    llm: LLM,
    example: Dict,
    decomposed_question: Optional[str] = None,
    demonstration_prompts: Optional[List[Tuple[str, str]]] = None,
    num_return_sequences: int = 1,
) -> Dict | None:
    # demonstration_prompts: List[(question_prompt, expected_response)]

    # Create prompt
    instruction_prompt, question_prompt, _ = prepare_prompt(
        example, decomposed_question=decomposed_question
    )

    # Tokenize
    if demonstration_prompts is not None:
        input_tokens = llm.few_shot_tokenize(
            instruction_prompt, question_prompt, demonstration_prompts
        )
    else:
        input_tokens = llm.tokenize(instruction_prompt, question_prompt)

    # Generate LLM predictions
    prediction = llm.generate(input_tokens, num_return_sequences=num_return_sequences)

    # Try to extract json objects from the LLM prediction
    if num_return_sequences > 1:
        json_predictions = [extract_json(p) for p in prediction]
        json_prediction = combine_json_predictions(json_predictions)
    else:
        json_prediction = extract_json(prediction)

    return json_prediction


def prepare_demonstration_prompts(
    llm: LLM,
    demonstrations: List[Dict],
    demonstration_rankings: List[int],
    num_of_demonstrations: int = 1,
) -> List[Tuple[str, str, str, str]]:
    # output: List[(question_prompt, expected_response)]
    max_demonstration_length = llm.get_max_length() - 1000

    demonstration_prompts = []
    for i in range(num_of_demonstrations, len(demonstrations)):
        # Pick the candidates
        candidate_demonstrations = [
            demonstrations[j]
            for j in demonstration_rankings[i : i + num_of_demonstrations]
        ]

        # Create their prompts
        candidate_prompts = []
        for demonstration in candidate_demonstrations:
            _, question_prompt, expected_response = prepare_prompt(demonstration)
            candidate_prompts.append((question_prompt, expected_response))
        # Check if their combined length can fit in the model's context
        candidate_prompts_len = sum(
            [len(question_prompt) for question_prompt, _ in candidate_prompts]
        )
        if candidate_prompts_len <= max_demonstration_length:
            return candidate_prompts

    # Pick k random demonstrations
    # NOTE: This might exceed the model's context length, if the previous approach fails fall back to this
    chosen_demonstrations = random.sample(demonstrations, k=num_of_demonstrations)
    for demonstration in chosen_demonstrations:
        _, question_prompt, expected_response = prepare_prompt(demonstration)

        demonstration_prompts.append((question_prompt, expected_response))

    return demonstration_prompts


def predict(
    model_name_or_path: str,
    examples: List[Dict],
    decompositions: List[List | None],
    demonstrations: List[Dict] | None = None,
    demonstration_rankings=None,
    num_of_demonstrations: Optional[int] = None,
    num_return_sequences: int = 1,
):
    # Load the model
    # NOTE: Switch to commented code if you want to use HF backend
    llm = VLLM(model_name_or_path)
    # if "llama" in model_name_or_path.lower():
    #     llm = Llama(model_name_or_path)
    # elif "deepseek" in model_name_or_path.lower():
    #     llm = DeepseekCoder(model_name_or_path)
    # elif "granite" in model_name_or_path.lower():
    #     llm = Granite(model_name_or_path)
    # elif "codestral" in model_name_or_path.lower():
    #     llm = Codestral(model_name_or_path)
    # else:
    #     llm = LLM(model_name_or_path)
    # llm.model.eval()

    predictions = []
    for i, (example, decomposition) in tqdm(
        enumerate(zip(examples, decompositions)), total=len(examples)
    ):

        # Prepare demonstrations if doing few-shot learning
        if demonstrations is not None:
            demonstration_prompts = prepare_demonstration_prompts(
                llm,
                demonstrations,
                demonstration_rankings=list(demonstration_rankings[i, :]),
                num_of_demonstrations=num_of_demonstrations,
            )
        else:
            demonstration_prompts = None

        if decomposition is not None and len(decomposition) < 20:
            # Generate a prediction for each decomposed question
            json_predictions = [
                get_prediction(
                    llm,
                    example,
                    decomposed_question=decomposed_question,
                    demonstration_prompts=demonstration_prompts,
                    num_return_sequences=num_return_sequences,
                )
                for decomposed_question in decomposition
            ]

            # Combine individual predictions
            json_prediction = combine_json_predictions(json_predictions)
        else:
            # If we don't have a decomposition get a single prediction
            json_prediction = get_prediction(
                llm,
                example,
                demonstration_prompts=demonstration_prompts,
                num_return_sequences=num_return_sequences,
            )

        predictions.append(json_prediction)

    return predictions


def parse_args():
    parser = argparse.ArgumentParser(description="Prepare examples for Text-to-SQL")
    parser.add_argument(
        "-i", "--input_file", help="The input .json file,", required=True
    )
    parser.add_argument(
        "-o",
        "--output_file",
        help="The output file to write the generated prediction.",
        required=True,
    )
    parser.add_argument(
        "-m",
        "--model",
        help="The LLM model to use for predicting.",
        required=True,
    )
    parser.add_argument(
        "--decomposed_file",
        help="A file containing the decompositions of the questions.",
        default=None,
    )
    parser.add_argument(
        "--demonstrations_file",
        help="A .json file containing Text-to-SQL demonstrations for few-shot prompting.",
        default=None,
    )
    parser.add_argument(
        "--num_of_demonstrations",
        help="The number of demonstrations to include in a few-shot setting.",
        type=int,
        default=0,
    )
    parser.add_argument(
        "--num_return_sequences",
        help="The number of demonstrations to include in a few-shot setting.",
        type=int,
        default=1,
    )

    args = parser.parse_args()
    return args


if __name__ == "__main__":
    args = parse_args()

    # Get Text-to-SQL examples
    examples = load_json(args.input_file)
    # Get decompositions (if available)
    decompositions = (
        load_json(args.decomposed_file)
        if args.decomposed_file is not None
        else [None] * len(examples)
    )

    if args.demonstrations_file is not None and args.num_of_demonstrations > 0:
        # Load demonstrations
        demonstrations = load_json(args.demonstrations_file)
        print("Calculating demonstration similarities...")
        # Calculate similarities of examples to demonstrations
        # TODO: Change hard-coded model path
        similarities = get_demonstration_silimarities(
            examples,
            demonstrations,
            sim_model_path="../models/princeton-nlp/sup-simcse-roberta-base",
        )
        # Create a ranking of most similar demonstrations for each example
        demonstration_rankings = np.argsort(similarities, axis=1)
        # Flip the sorted rankings to get a descending ranking (i.e., most similar first)
        demonstration_rankings = np.flip(demonstration_rankings, axis=1)

        # shuffled_indexes = list(range(0, len(demonstrations)))
        # random.shuffle(shuffled_indexes)
    else:
        demonstrations = None
        demonstration_rankings = None

    # Get predictions
    print("Starting schema linking predictions...")
    predictions = predict(
        args.model,
        examples,
        decompositions,
        demonstrations,
        demonstration_rankings,
        args.num_of_demonstrations,
        num_return_sequences=args.num_return_sequences,
    )
    # Write predictions
    write_json(predictions, args.output_file)
