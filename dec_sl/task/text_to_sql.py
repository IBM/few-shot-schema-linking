import argparse
from typing import List, Dict, Tuple, Optional
from tqdm.auto import tqdm
import numpy as np
import random

from dec_sl.llm.llm_base import LLM
from dec_sl.llm.llama import Llama
from dec_sl.llm.deepseek import DeepseekCoder
from dec_sl.llm.granite import Granite
from dec_sl.llm.codestral import Codestral
from dec_sl.llm.vllm_base import VLLM

from dec_sl.processing.prompting import create_text_to_sql_prompt
from dec_sl.processing.postprocess import (
    extract_fenced_code_block,
    remove_extra_whitespace,
    extract_labels_from_json,
    refine_schema_link_predictions,
    group_column_labels_by_table,
)
from dec_sl.utils.text_utils import preprocess_evidence, get_demonstration_silimarities
from dec_sl.utils.db_utils import get_matched_content_sequence, get_db_schema_sequence
from dec_sl.utils.file_utils import load_json


def prepare_prompt(
    example: Dict,
    decomposed_question: Optional[str] = None,
    table_labels: Optional[List[int]] = None,
    column_labels: Optional[List[List[int]]] = None,
) -> Tuple[str, str, str]:
    """Create the LLM prompts for a given example."""
    # Get all necessary data
    if decomposed_question is None:
        nl_question = example["question"]
    else:
        nl_question = decomposed_question

    serialized_schema = get_db_schema_sequence(
        example["schema"],
        include_fk=True,
        include_content=True,
        table_labels=table_labels,
        column_labels=column_labels,
    )
    evidence = preprocess_evidence(example["evidence"])
    matched_content_sequence = get_matched_content_sequence(example["matched_contents"])

    # Add evidence
    # NOTE: Maybe add this to the prompt?
    if evidence != "":
        nl_question = f"{nl_question} (Notes: {evidence})"

    # Add matched contents
    # NOTE: Maybe add this to the prompt?
    # NOTE: Maybe this could be phrased in a better way?
    if matched_content_sequence != "":
        nl_question = f"{nl_question}\nAlso take into account the following {matched_content_sequence}"

    # Create input prompts
    instruction_prompt, question_prompt = create_text_to_sql_prompt(
        serialized_schema, nl_question
    )

    # Create expected response as it should be generated by the LLM
    expected_response = f"```sql\n{example['sql']}\n```"

    return instruction_prompt, question_prompt, expected_response


def get_prediction(
    llm: LLM,
    example: Dict,
    decomposed_question: Optional[str] = None,
    demonstration_prompts: Optional[List[Tuple[str, str]]] = None,
    num_return_sequences: int = 1,
    table_labels: Optional[List[int]] = None,
    column_labels: Optional[List[List[int]]] = None,
) -> Dict | None:
    # demonstration_prompts: List[(question_prompt, expected_response)]

    # Create prompt
    instruction_prompt, question_prompt, _ = prepare_prompt(
        example,
        decomposed_question=decomposed_question,
        table_labels=table_labels,
        column_labels=column_labels,
    )

    # Tokenize
    if demonstration_prompts is not None:
        input_tokens = llm.few_shot_tokenize(
            instruction_prompt, question_prompt, demonstration_prompts
        )
    else:
        input_tokens = llm.tokenize(instruction_prompt, question_prompt)

    # Generate LLM predictions
    prediction = llm.generate(input_tokens, num_return_sequences=num_return_sequences)

    # Try to extract SQL from the LLM prediction
    sql_prediction = extract_fenced_code_block(prediction, "sql")

    if sql_prediction is not None:
        sql_prediction = remove_extra_whitespace(sql_prediction)

    return sql_prediction


def prepare_demonstration_prompts(
    llm: LLM,
    demonstrations: List[Dict],
    demonstration_rankings: List[int],
    num_of_demonstrations: int = 1,
) -> List[Tuple[str, str, str, str]]:
    # output: List[(question_prompt, expected_response)]
    max_demonstration_length = llm.get_max_length() - 1000

    demonstration_prompts = []
    for i in range(num_of_demonstrations, len(demonstrations)):
        # Pick the candidates
        candidate_demonstrations = [
            demonstrations[j]
            for j in demonstration_rankings[i : i + num_of_demonstrations]
        ]

        # Create their prompts
        candidate_prompts = []
        for demonstration in candidate_demonstrations:
            _, question_prompt, expected_response = prepare_prompt(demonstration)
            candidate_prompts.append((question_prompt, expected_response))
        # Check if their combined length can fit in the model's context
        candidate_prompts_len = sum(
            [len(question_prompt) for question_prompt, _ in candidate_prompts]
        )
        if candidate_prompts_len <= max_demonstration_length:
            return candidate_prompts

    # Pick k random demonstrations
    # NOTE: This might exceed the model's context length, if the previous approach fails fall back to this
    chosen_demonstrations = random.sample(demonstrations, k=num_of_demonstrations)
    for demonstration in chosen_demonstrations:
        _, question_prompt, expected_response = prepare_prompt(demonstration)

        demonstration_prompts.append((question_prompt, expected_response))

    return demonstration_prompts


def predict(
    llm: LLM,
    examples: List[Dict],
    schema_linking_labels: Optional[List[Tuple[List, List]]] = None,
    demonstrations: Optional[List[Dict]] = None,
    demonstration_rankings=None,
    num_of_demonstrations: Optional[int] = None,
):
    predictions = []

    for i, example in tqdm(enumerate(examples), total=len(examples)):
        # Prepare demonstrations if doing few-shot learning
        if demonstrations is not None:
            demonstration_prompts = prepare_demonstration_prompts(
                llm,
                demonstrations,
                demonstration_rankings=list(demonstration_rankings[i, :]),
                num_of_demonstrations=num_of_demonstrations,
            )
        else:
            demonstration_prompts = None

        # Generate prediction
        if schema_linking_labels is not None:
            table_labels, column_labels = schema_linking_labels[i]
        else:
            table_labels, column_labels = None, None

        prediction = get_prediction(
            llm,
            example,
            demonstration_prompts=demonstration_prompts,
            table_labels=table_labels,
            column_labels=column_labels,
        )

        predictions.append(prediction)

    return predictions


def parse_args():
    parser = argparse.ArgumentParser(description="Prepare examples for Text-to-SQL")
    parser.add_argument(
        "-i", "--input_file", help="The input .json file,", required=True
    )
    parser.add_argument(
        "--schema_links_file",
        help="A .json file containing schema linking predictions.",
        default=None,
    )
    parser.add_argument(
        "--demonstrations_file",
        help="A .json file containing Text-to-SQL demonstrations for few-shot prompting.",
        default=None,
    )
    parser.add_argument(
        "--num_of_demonstrations",
        help="The number of demonstrations to include in a few-shot setting.",
        type=int,
        default=0,
    )
    parser.add_argument(
        "-o",
        "--output_file",
        help="The output file to write the generated prediction.",
        required=True,
    )
    parser.add_argument(
        "-m",
        "--model",
        help="The LLM model to use for predicting.",
        required=True,
    )

    args = parser.parse_args()
    return args


if __name__ == "__main__":
    args = parse_args()

    # Get Text-to-SQL examples
    examples = load_json(args.input_file)

    if args.schema_links_file is not None:
        # Load schema linking preditions
        schema_linking_predictions = load_json(args.schema_links_file)
        schema_linking_labels = []
        for json_prediction, example in zip(schema_linking_predictions, examples):
            # Convert preditions into binary labels
            table_labels, column_labels, _, _ = extract_labels_from_json(
                json_prediction, example["schema"]["schema_items"], fuzzy_matching=True
            )
            # Improve predictions through automatic refinement
            table_labels, column_labels = refine_schema_link_predictions(
                table_labels, column_labels, example["schema"]
            )
            # Group column labels to be in the same format as stored in the dataset files
            column_labels = group_column_labels_by_table(
                column_labels, example["schema"]["schema_items"]
            )

            schema_linking_labels.append((table_labels, column_labels))
    else:
        schema_linking_labels = None

    # Load demonstrations and compute similarities
    if args.demonstrations_file is not None and args.num_of_demonstrations > 0:
        # Load demonstrations
        demonstrations = load_json(args.demonstrations_file)
        print("Calculating demonstration similarities...")
        # Calculate similarities of examples to demonstrations
        # TODO: Change hard-coded model path
        similarities = get_demonstration_silimarities(
            examples,
            demonstrations,
            sim_model_path="../models/princeton-nlp/sup-simcse-roberta-base",
        )
        # Create a ranking of most similar demonstrations for each example
        demonstration_rankings = np.argsort(similarities, axis=1)
        # Flip the sorted rankings to get a descending ranking (i.e., most similar first)
        demonstration_rankings = np.flip(demonstration_rankings, axis=1)
    else:
        demonstrations = None
        demonstration_rankings = None

    # Load the model
    model_name_or_path = args.model
    # NOTE: Switch to commented code if you want to use HF backend
    llm = VLLM(model_name_or_path)
    # if "llama" in model_name_or_path.lower():
    #     llm = Llama(model_name_or_path)
    # elif "deepseek" in model_name_or_path.lower():
    #     llm = DeepseekCoder(model_name_or_path)
    # elif "granite" in model_name_or_path.lower():
    #     llm = Granite(model_name_or_path)
    # elif "codestral" in model_name_or_path.lower():
    #     llm = Codestral(model_name_or_path)
    # else:
    #     llm = LLM(model_name_or_path)

    # Get predictions
    predictions = predict(
        llm,
        examples,
        schema_linking_labels=schema_linking_labels,
        demonstrations=demonstrations,
        demonstration_rankings=demonstration_rankings,
        num_of_demonstrations=args.num_of_demonstrations,
    )
    # Write predictions in evaluation format
    with open(args.output_file, "w") as fp:
        for example, prediction in zip(examples, predictions):
            db_id = example["db_id"]
            fp.write(f"{prediction}\t{db_id}\n")
